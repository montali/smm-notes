\subsection{Vector spaces }

\begin{definition}
    A \textit{vector space} over a field $F$ ($F = \set{R}$ or $F = \set{C}$) is a set closed under finite \textit{vector addition} and \textit{scalar multiplication}. The elements of $V$ are called \textit{vectors} while the elements of $F$ are called \textit{scalars}. The two operations must satisfy the following properties:
    
    \begin{enumerate}
        \item Commutativity of addition
        $$ \forall \vec{v}, \vec{w} \in V,\ \vec{v} + \vec{w} = \vec{w} + \vec{v} $$
        \item Associativity of addition
        $$ \forall \vec{u}, \vec{v}, \vec{w} \in V,\ \vec{u} + (\vec{v} + \vec{w}) = (\vec{u} + \vec{v}) + \vec{w}$$
        \item Existence of the identity element of addition, $\vec{0} \in V$, such that
        $$ \forall \vec{v} \in V,\ \vec{v} + \vec{0} = \vec{v} $$
        \item Existence of the additive inverse
        $$ \forall \vec{v} \in V,\ \exists \mathbf{-v} \in V,\ \vec{v} + (\mathbf{-v}) = \vec{0} $$
        \item Existence of the identity element of scalar multiplication, $1 \in F$, such that
        $$ \forall \vec{v} \in V,\ 1\vec{v} = \vec{v} $$
        \item Compatibility of scalar multiplication with field multiplication
        $$ \forall a, b \in F,\ \forall \vec{v} \in V,\ (ab)\vec{v} = a(b\vec{v}) $$
        \item Distributivity of scalar multiplication with respect to field addition
        $$ \forall a, b, \in F,\ \forall \vec{v} \in V,\ (a + b)\vec{v} = a\vec{v} + b\vec{v} $$
        \item Distributivity of scalar multiplication with respect to vector addition
        $$ \forall a \in F,\ \forall \vec{v}, \vec{w} \in V,\ a(\vec{v} + \vec{w}) = a\vec{v} + a\vec{w} $$
    \end{enumerate}
\end{definition}

\input{examples/section-2/notable-vector-spaces.tex}

\begin{definition}
    Given $V$ vector space over the field $F$, the set $W$ is a \textit{subspace} of $V$ if and only if:
    \begin{itemize}
        \item $W \subset V$
        \item $W$ is a vector space over $F$
    \end{itemize}
\end{definition}

\input{examples/section-2/subspace-1.tex}

\input{examples/section-2/subspace-2.tex}

\subsubsection{Linear independence}

\begin{definition}
    Given a vector space $V$ over $F$, the set $W$ of all finite linear combinations of vectors $\{\vec{v}_1, \hdots, \vec{v}_m\},\ \vec{v}_i \in V$, is called the \textit{subspace spanned by $\{\vec{v}_1, \hdots, \vec{v}_m\},\ \vec{v}_i$} and it is written as
    $$ W = \text{span}\{\vec{v}_1, \hdots, \vec{v}_m\} = \left\{ \left.\sum_{i=1}^{m}{\alpha_i\vec{v}_i} \ \right\vert \vec{v}_i \in V,\ \alpha_i \in F \right\} $$
    The system $\{\vec{v}_1, \hdots, \vec{v}_m\}$ is called a \textit{system of generators} for $V$.
\end{definition}

\input{examples/section-2/generators.tex}

\begin{definition}
    Given a vector space $V$ over $F$, a system of vectors\\ $\{\vec{v}_1, \hdots, \vec{v}_m\},\ \vec{v}_i \in V$ is called \textit{linearly independent} if
    $$ \alpha_1\vec{v}_1 + \hdots + \alpha_m\vec{v}_m = \vec{0} \implies \alpha_1 = \alpha_2 = \hdots = \alpha_m = 0 $$
    with $\alpha_1, \hdots, \alpha_m \in F$. Otherwise, the system is called \textit{linearly dependent}.
\end{definition}

From a geometrical point of view, $n$ vectors are linearly dependent if they lie on the same $(n-1)$-dimensional hyperplane.

\input{examples/section-2/linear-independence.tex}

\begin{definition}
    We call a \textit{basis} for a vector space $V$ any system of linearly independent generators of $V$.
\end{definition}

\input{examples/section-2/basis.tex}

\begin{proposition}
    Let $V$ be a vector space which admits a basis of $n$ vectors. Then every system of linearly independent vectors has at most $n$ elements and any other basis of $V$ has exactly $n$ elements. The number $n$ is called \textit{dimension} of V and is denoted by $\text{dim}(V) = n$.
\end{proposition}

\subsection{Matrices}

\begin{definition}
    Let $m$ and $n$ be two positive integers. We call \textit{matrix} the rectangular array having $m$ rows and $n$ columns of elements in a field $F$. It is represented as
    $$
        \mat{A} = \left[\begin{matrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn}
        \end{matrix}\right]
    $$
    If the $F = \set{R}$ (respectively $F = \set{C}$) we write $\mat{A} \in \set{R}^{m \times n}$ (respectively $\mat{A} \in \set{C}^{m \times n})$. If $m = n$ the matrix is called \textit{square}. The set of entries $A_{ij}$ with $i = j$ is called \textit{main diagonal} of the matrix $\mat{A}$.
\end{definition}

We can write the above matrix as $A(m \times n)$ or $A = (a_{ij})$, $i = 1, \hdots, m$ and $j = 1, \hdots, n$.

\begin{definition}
    Let $\mat{A} \in F^{m \times n}$. The maximum number of linearly independent columns (or rows) of $\mat{A}$ is called \textit{rank}, denoted by $\rank{A}$. A is said to have \textit{complete} or \textit{full rank} if $\rank{A} = \min(m, n)$.
\end{definition}

\begin{definition}
    A \textit{lower triangular matrix} is a matrix $\mat{L}$ where $l_{ij} = 0$ if $i < j$. An \textit{upper triangular matrix} is a matrix $\mat{U}$ where $u_{ij} = 0$ if $i > j$.
    $$
        \mat{L} = \left[\begin{matrix}
        l_{11} & 0 & \cdots & 0 \\
        l_{21} & l_{22} & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        l_{n1} & l_{n2} & \cdots & l_{nn}
        \end{matrix}\right],\  
        \mat{U} = \left[\begin{matrix}
        u_{11} & u_{12} & \cdots & u_{1n} \\
        0 & u_{22} & \cdots & u_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & u_{nn}
        \end{matrix}\right]
    $$
\end{definition}

\subsubsection{Operations with matrices}

Let $\mat{A} \in \set(R)^{m \times p}$, $\mat{B} \in \set(R)^{m \times p}$, $\mat{C} \in \set(R)^{p \times n}$, $\lambda \in F$. We define the following operations:

\begin{itemize}
    \item \textit{matrix addition}
    $$ \mat{A} + \mat{B} = (a_{ij} + b_{ij}),\ \mat{A} + \mat{B} \in \set(R)^{m \times p} $$
    The identity element of matrix addition is the \textit{null matrix}, denoted by $\mat{0}$ and made up only by null entries;
    \item \textit{matrix multiplication by a scalar}
    $$ \lambda\mat{A} = (\lambda a_{ij}),\ \lambda\mat{A} \in \set(R)^{m \times p} $$
    \item \textit{matrix multiplication}
    $$ \mat{A}\mat{C} = \left(\sum_{k = 1}^{p}a_{ik}b_{kj}\right),\ \mat{A}\cdot \mat{C} \in \set(R)^{m \times n} $$
    Notice how matrix multiplication is defined only when the number of columns of the first matrix is equal to the number of rows of the second matrix. This operation results in a matrix with size $(m, n)$. Matrix product is associative and distributive with respect to matrix addition, but it is not in general commutative
    $$ \mat{A} \cdot \mat{C} \neq \mat{C}\cdot \mat{A} $$
    We call \textit{commutative} the square matrices for which $\mat{A}\cdot \mat{C} = \mat{C}\mat{A}$ holds.
    \item \textit{transposition}
    $$ \transp{A} = (a_{ji}),\ \transp{A} \in F^{p \times m} $$
    Transposition enjoys the following properties:
    $$ \transp{(\transp{A})} = A,\ \transp{(A + B)} = \transp{A} + \transp{B},\ \transp{(AC)} = \transp{C}\transp{A},\ \transp{(\lambda\mat{A})} = \lambda\transp{A} $$
\end{itemize}

\input{examples/section-2/mat-ops.tex}

\begin{definition}
    A \textit{diagonal matrix} of order $n$ is a square matrix of the type $A = (d_{ii}\delta_{ij})$, where $\delta_{ij}$ denotes the \textit{Kronecker symbol} equal to $1$ if $i = j$ and $0$ otherwise. It can be written as $A = \diag{d_{11}, d_{22}, \hdots, d_{nn}}$ or $A = \diag{\vec{d}}$, where $\vec{d} = (d_{11}, d_{22}, \hdots, d_{nn})$.
\end{definition}

\begin{definition}
    We call \textit{identity matrix of order $n$} the square matrix of size $(n, n)$ given by $\mat{I}_n = \diag{\vec{1}}$. Usually, we write the identity matrix as $\mat{I}$.
    $$
        \mat{I} = \left[\begin{matrix}
        1 & 0 & \cdots & 0 \\
        0 & 1 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 1
        \end{matrix}\right]
    $$
    The identity matrix is, by definition, the only matrix such that $\mat{A}\mat{I}_n = \mat{I}_n\mat{A} = \mat{A}$, for all $\mat{A}(n \times n)$.
\end{definition}

\begin{definition}
    A matrix $\mat{A}(n \times n)$ is called \textit{invertible} (or \textit{nonsingular}) if there exists a matrix $\mat{B}(n \times n)$ such that $\mat{A}\mat{B} = \mat{B}\mat{A} = \mat{I}$. $\mat{B}$ is called \textit{inverse matrix} of $\mat{A}$ and is denoted by $\inv{A}$. A non-invertible square matrix is called \textit{singular}.
    
    The inverse of an invertible matrix is also invertible, $\inv{(\inv{A})} = \mat{A}$, and if two square matrices $\mat{A}$ and $\mat{B}$ are both invertible, their product is also invertible, $\inv{(AB)} = \inv{B}\inv{A}$. Moreover, if a square matrix $\mat{A}$ is invertible, then $\inv{(\transp{A})} = \transp{(\inv{A})} = \mat{A}^{-T}$
\end{definition}

\begin{proposition}
    A square matrix is invertible if its column vectors are linearly independent.
\end{proposition}

\input{examples/section-2/inverse-matrix-1.tex}

\begin{definition}
    A square matrix $\mat{A}$ is called \textit{symmetric} if $\mat{A} = \transp{A}$, \textit{antisymmetric} if $\mat{A} = -\transp{A}$, \textit{orthogonal} if $\inv{A} = \transp{A}$, i.e. $\mat{A}\transp{A} = \transp{A}\mat{A} = \mat{I}$.
\end{definition}

\input{examples/section-2/symmetric-antisymmetric-orthogonal.tex}

\subsubsection{Determinant of a matrix}

\begin{definition}
    Let $\mat{A} \in \set(C)^{n \times n}$. We call the \textit{determinant} of $\mat{A}$, denoted by $\det{A}$, the scalar defined through the following formula
    $$
        \det{(\mat{A})} = \left\{\begin{matrix}
        a_{11} & \text{if}\ n = 1 \\
        \sum_{j=1}^{n}{(-1)^{i+j}\det{(\mat{A}_{ij})}a_{ij}} = \sum_{i=1}^{n}{(-1)^{i+j}\det{(\mat{A}_{ij})}a_{ij}} & \text{if}\ n > 1
        \end{matrix}\right.
    $$
    with $i \in \{1, \hdots, n\}$ (or $j \in \{1, \hdots, n\}$) fixed, where $\mat{A}_{ij}$ is the submatrix of order $n - 1$ obtained from $A$ by eliminating row $i$ and column $j$. This is known as \textit{Laplace's rule}
\end{definition}

If $\mat{A}$ is diagonal or triangular, then
$$\det{(\mat{A})} = \prod_{i=1}^{n}{a_{ii}}$$
The determinant enjoys the following properties
$$ \det{(\mat{A})} = \det{(\transp{A})},\ \det{(\mat{AB})} = \det{(\mat{A})}\det{(\mat{B})},\ \det{(\inv{A})} = \frac{1}{\det{(\mat{A})}}, $$
$$ \det{(\alpha\mat{A})} = \alpha^{n}\det{(\mat{A})},\ \forall \alpha \in F $$

\begin{proposition}
    Every orthogonal matrix $\mat{A}$ is invertible and
    $$ \det{(\mat{A})} = \pm 1 $$
\end{proposition}

In the following examples we illustrate methods to compute a matrix determinant that do not require the use of Laplace's rule.

\input{examples/section-2/det-2x2.tex}

\input{examples/section-2/det-3x3.tex}

\input{examples/section-2/det-4x4.tex}

\begin{proposition}
    If $\mat{A}$ is invertible then
    $$ \inv{A} = \frac{\transp{C}}{\det{(\mat{A})}} $$
    where $\mat{C}$ is the \textit{cofactor matrix} having elements $c_{ij} = (-1)^{i+j}\det{(\mat{A}_{ij})}$, called \textit{cofactors}.
\end{proposition}

\begin{proposition}
    For a matrix $\mat{A} \in \set{C}^{n \times n}$ the following properties are equivalent:
    \begin{itemize}
        \item $\mat{A}$ is nonsingular
        \item $\det{(\mat{A})} \neq 0$
        \item $\rank{A} = n$
        \item $\mat{A}$ has linearly independent columns and rows
    \end{itemize}
\end{proposition}

\input{examples/section-2/inverse-matrix-2.tex}

\subsubsection{Eigenvalues and eigenvectors}

\begin{definition}
    Let $\mat{A} \in \set{C}^{n\times n}$. The number $\lambda \in \set{C}$ is called an \textit{eigenvalue} of $\mat{A}$ if
    $$ \exists \vec{x} \in \set{C}^n,\ \vec{x} \neq \vec{0}\ \text{such that } \mat{A}\vec{x} = \lambda\vec{x} $$
    The vector $\vec{x}$ is called the \textit{eigenvector} associated with the eigenvalue $\lambda$. The set of eigenvalues of $\mat{A}$ is called \textit{spectrum} of $\mat{A}$ and is denoted by $\sigma(\mat{A})$. The eigenvalues of $\mat{A}$ are the solutions of the \textit{characteristic equation}
    $$ p_{\mat{A}}(\lambda) = \det{(\mat{A} - \lambda\mat{I})} = 0. $$
    where $p_{\mat{A}}(\lambda)$ is called \textit{characteristic polynomial}. For the fundamental theorem of algebra the characteristic polynomial, which has degree $n$, has exactly $n$ solutions. Hence, a matrix with real or complex entries has $n$ eigenvalues, counted with their multiplicity.
\end{definition}

\begin{definition}
Let a square matrix $\mat{A}$ have an eigenvalue $\lambda_i$. The algebraic multiplicity of $\lambda_i$ is the number of times the root appears in the characteristic polynomial.
\end{definition}

\begin{definition}
    The maximum eigenvalue in module of a matrix $\mat{A} \in \set{C}^{n \times n}$ is called the \textit{spectral radius} of $\mat{A}$ and is denoted by
    $$ \rho(\mat{A}) = \max_{\lambda \in \sigma(\mat{A})}{\left\vert \lambda \right\vert} $$
\end{definition}

\textit{Remark.} The eigenvectors of a matrix $\mat{A}$ are not unique. If $\vec{x}$ is an eigenvector of $\mat{A}$ associated with an eigenvalue $\lambda$, then for any $c \in \mathbf{R}\ \{0\}$ $c\vec{x}$ is an eigenvector of $\mat{A}$ with the same eigenvalue.
$$\mat{A} (c \vec{x})=c \mat{A} \vec{x}=c \lambda \vec{x}= \lambda (c\vec{x})$$

\begin{proposition}
    Let $\mat{A} \in \set{C}^{n \times n}$ with eigenvalues $\lambda_1, \hdots, \lambda_n$, then:
    $$ \det{(\mat{A})} = \prod_{i=1}^{n}{\lambda_i} $$
\end{proposition}

\begin{proposition}
    $\mat{A}$ matrix is singular iff it has at least one null eigenvalue so $\mat{A}^{-1}$ doesn't exists.
\end{proposition}

\begin{proposition}
    Let $\mat{A} \in \set{C}^{n \times n}$ with eigenvalues $\lambda_1, \hdots, \lambda_n$. If $\mat{A}$ is diagonal or triangular, then $\lambda_i = a_{ii}$, $i = 1, \hdots, n$.
\end{proposition}

\begin{proposition}
 A symmetric positive (semi)definite matrix $A$ has eigenvalues $\lambda_i$ greater (equal) than zero.
 \end{proposition}
 
\begin{definition}
    Two matrices $\mat{A} \in \set{C}^{n \times n}$ and $\mat{B} \in \set{C}^{n \times n}$ are said \textit{similar} if there exists a non singular matrix $\mat{P}$ so that $\mat{B} = \mat{PA}\inv{P}$.
   \end{definition}

\begin{property}
    $\mat(A)$ and $\mat(B)$ are similar iff they have the same eigenvalues. 
\end{property}

Example 4.5 in MML.
\subsection{Norms in Vector Spaces}

%\begin{definition}
%    A \textit{scalar product} on a vector space $V$ over the field $F$ is a mapping $\langle\cdot,\cdot\rangle:\ V \times V \rightarrow F$ which enjoys the following properties:
 %   \begin{itemize}
 %       \item $\langle \lambda \vec{x} + \gamma \vec{y}, \vec{z} \rangle = \lambda\langle \vec{x}, \vec{z} \rangle + \gamma\langle \vec{y}, \vec{z} \rangle,\ \forall \vec{x}, %\vec{y}, \vec{z} \in V,\ \forall \lambda, \gamma \in F$
 %       \item $\langle \vec{x}, \vec{y} \rangle = \conj{\langle \vec{y}, \vec{x} \rangle},\ \forall \vec{x}, \vec{y} \in V$
 %       \item $\langle \vec{x}, \vec{x} \rangle \geq 0,\ \forall \vec{v} \in V$ and $\langle \vec{x}, \vec{x} \rangle = 0 \iff \vec{x} = \vec{0}$
 %   \end{itemize}
%\end{definition}

%A notable example of scalar product is the \textit{Euclidean scalar product}
%$$ \langle \vec{x}, \vec{y} \rangle = \sum_{i=1}^{n}{x_iy_i},\ \forall \vec{x}, \vec{y} \in \set{R}^n $$

\begin{definition}
    Let $V$ be a vector space over the field $F$. We define the map $\norm{\cdot}: V \rightarrow F$ as a \textit{norm} on $V$ if the following properties are satisfied:
    \begin{itemize}
        \item $\norm{\vec{v}} \geq 0,\ \forall \vec{v} \in V$ and $\norm{\vec{v}} = 0 \iff \vec{v} = \vec{0}$
        \item $\norm{\alpha\vec{v}} = |\alpha|\norm{\vec{v}},\ \forall \alpha \in F,\ \forall \vec{v} \in V$
        \item $\norm{\vec{v} + \vec{w}} \leq \norm{\vec{v}} + \norm{\vec{w}},\ \forall \vec{v}, \vec{w} \in V$
    \end{itemize}
    where $|\alpha|$ denotes the absolute value of $\alpha$ if $F = \set{R}$, the module of $\alpha$ if $F = \set{C}$. $(V, \norm{\cdot})$ is called a \textit{normed space}.
\end{definition}

\input{examples/section-2/vec-norm-euclidean.tex}

\input{examples/section-2/vec-norm-manhattan.tex}

\input{examples/section-2/vec-norm-p.tex}

\input{examples/section-2/vec-norm-infinity.tex}
\begin{definition}
    Two norms $\|{\cdot}\|_p $ and $\|{\cdot}\|_q $ are said \textit{equivalent} if two positive constants $c_{pq}$ and $C_{pq}$, defined as follows, exist:
    $$c_{pq} \|\vec{x}\|_q\leq \|\vec{x}\|_p \leq C_{pq}\|\vec{x}\|_q \forall \vec{x} \in V$$
\end{definition}

\textit{Result.} In a vector space all the $p$ norms are equivalent.

\subsubsection{Matrix norms}

\begin{definition}
    A \textit{matrix norm} is a function $\norm{\cdot}: \set{R}^{m \times n} \rightarrow \set{R}$ satisfying the following properties:
    \begin{itemize}
        \item $\norm{\mat{A}} \geq 0,\ \forall \mat{A} \in \set{R}^{m \times n}$ and $\norm{\mat{A}} = 0 \iff \mat{A} = \mat{0}$
        \item $\norm{\alpha\mat{A}} = |\alpha|\norm{\mat{A}},\ \forall \alpha \in \set{R},\ \forall \mat{A} \in \set{R}^{m \times n}$
        \item $\norm{\mat{A} + \mat{B}} \leq \norm{\mat{A}} + \norm{\mat{B}},\ \forall \mat{A}, \mat{B} \in \set{R}^{m \times n}$
    \end{itemize}
\end{definition}

\begin{definition}
    Let $\mat{A} \in \set{R}^{n \times n}$. We say the matrix norm $\norm{\cdot}$ is compatible with a vector norm $\norm{\cdot}$ if
    $$\norm{\mat{A}\vec{x}} \leq \norm{\mat{A}}\norm{\vec{x}}$$
\end{definition}

\input{examples/section-2/mat-norm-spectral.tex}

\input{examples/section-2/mat-norm-1.tex}

\input{examples/section-2/mat-norm-infinity.tex}

\input{examples/section-2/mat-norm-frobenius.tex}

 
